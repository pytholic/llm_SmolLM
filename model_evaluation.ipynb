{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    " \n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    " \n",
    "ollama_running = check_if_running(\"ollama\")\n",
    " \n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load fine-tuned model responses \n",
    "file_path = \"instruction-data-with-response.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    " \n",
    "# # Apply alpaca prompt style\n",
    "# def format_input(entry):\n",
    "#     instruction_text = (\n",
    "#         f\"Below is an instruction that describes a task. \"\n",
    "#         f\"Write a response that appropriately completes the request.\"\n",
    "#         f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "#     )\n",
    "#     input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "#     return instruction_text + input_text\n",
    "\n",
    "# Apply phi-3 prompt style\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"<|user|>\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = f\"\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text\n",
    "\n",
    "# # Apply smollm prompt style\n",
    "# def format_input(entry):\n",
    "#     input_text = (\n",
    "#         \"<|im_start|>user\\n\"\n",
    "#         f\"{entry['instruction']} \\n{entry['input']}\"\n",
    "#         \"<|im_end|>\"\n",
    "#         )\n",
    "#     return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet consists of:\n",
      "\n",
      "1. **Grasses**: Various types of grasses, including tall grasses, short grasses, and grassy weeds.\n",
      "2. **Leaves**: Leaves from trees and shrubs, such as willow, alder, and oak.\n",
      "3. **Hay**: High-quality hay, like timothy or alfalfa hay, is a staple in many llama diets.\n",
      "4. **Fruits**: Fresh fruits, like apples, berries, and melons, are also consumed by llamas.\n",
      "5. **Grains**: Some llamas may be fed grains, such as oats or corn, but this should not make up more than 10-20% of their diet.\n",
      "6. **Browse**: Browse refers to the leaves, twigs, and branches of shrubs and trees. Llamas love to browse on plants like willow, alder, and cedar.\n",
      "\n",
      "In general, a llama's diet should consist of:\n",
      "\n",
      "* 50-60% hay\n",
      "* 20-30% grasses and other forages (like browse)\n",
      "* 10-20% grains (if fed at all)\n",
      "* 5-10% fruits and treats\n",
      "\n",
      "It's essential to provide llamas with high-quality food, fresh water, and a balanced diet to ensure their overall health and well-being.\n"
     ]
    }
   ],
   "source": [
    "# Ollama with REST API\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "MODEL = \"llama3\"\n",
    "\n",
    "def query_model(\n",
    "    prompt, \n",
    "    model=MODEL, \n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {     # Settings below are required for deterministic responses\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(\n",
    "        url, \n",
    "        data=payload, \n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "result = query_model(\"What do Llamas eat?\", MODEL)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "\n",
      "Score:\n",
      ">> I would score the model response 80 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The model correctly identifies that the input sentence is asking for a simile (a comparison using \"as\" or \"like\") and generates one.\n",
      "* The model chooses a relevant and well-known example of speed, which is a cheetah.\n",
      "* However, the model response could be improved by making it more vivid and engaging. For example, saying \"The car is as fast as a cheetah on steroids\" would add more humor and emphasis to the comparison.\n",
      "\n",
      "Overall, the model response is good but not perfect. It demonstrates an understanding of the task and generates a reasonable simile, but could benefit from further refinement and creativity.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud that is typically associated with thunderstorms is cumulus.\n",
      "\n",
      "Score:\n",
      ">> I would score this model response as 60.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The model has correctly identified that the question is asking about clouds associated with thunderstorms.\n",
      "* However, it incorrectly identifies the type of cloud as \"cumulus\", when in fact the correct answer is \"cumulonimbus\".\n",
      "* Cumulus clouds are indeed a type of cloud, but they are typically associated with fair weather and not thunderstorms.\n",
      "\n",
      "The score of 60 reflects that the model has some understanding of the question, but makes an error in its response. A perfect score would be 100, indicating a completely accurate response.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> I would score this model response a 90.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The model correctly identifies the author of \"Pride and Prejudice\" as Jane Austen.\n",
      "* The response is concise and directly answers the question.\n",
      "* However, the response could be slightly improved by removing the phrase \"The author of 'Pride and Prejudice' is\", which is not necessary to convey the answer. A more direct and simple response would be more desirable.\n",
      "\n",
      "Overall, a 90 score indicates that the model has a good understanding of the question and provides a correct answer, but could benefit from some minor improvements in terms of concision and simplicity.\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluating first three responses with llama3\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 110/110 [00:48<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 75.38\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Scoring all responses\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_model_scores(json_data, json_key, model=MODEL):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only. \" \n",
    "            f\"Do not add any text before or after the integer score. \"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 epochs\n",
    "\n",
    "SmolLM-360M-alpaca-prompt-style `score: 73.44`<br>\n",
    "SmolLM-360M-smollm-prompt-style`score: 73.14`<br>\n",
    "SmolLM-360M-phi3-prompt-style`score: 75.09`<br>\n",
    "SmolLM-360M-instruct-alpaca-prompt-style `score: 74.90`<br>\n",
    "SmolLM-360M-instruct-smollm-prompt-style `score: 71.75`<br>\n",
    "SmolLM-360M-instruct-phi3-prompt-style `score: 75.15`<br>\n",
    "SmolLM-360M-instruct-lora-phi3-prompt-style `score: 74.64`<br>\n",
    "\n",
    "SmolLM-135M-instruct `score: 63.84`<br>\n",
    "SmolLM-135M-instruct-lora `score: `<br>\n",
    "SmolLM-135M `score: 64.30`<br>\n",
    "SmolLM-135M-lora `score: `<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
