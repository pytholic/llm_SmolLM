{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install transformers\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "\n",
    "# device = \"cpu\" # for GPU usage or \"cpu\" for CPU usage\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", cache_dir=\"./.cache\")\n",
    "\n",
    "# # for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, cache_dir=\"./.cache\", max_length=100).to(device)\n",
    "\n",
    "# # messages = [{\"role\": \"user\", \"content\": \"List the steps to bake a chocolate cake from scratch.\"}]\n",
    "# # input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "# input_text = format_input(data[50])\n",
    "# print(input_text)\n",
    "# inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "# # outputs = model(inputs)\n",
    "# # print(outputs)\n",
    "# outputs = model.generate(inputs, max_new_tokens=100, temperature=0.6, top_p=0.92, do_sample=True, output_scores=True, return_dict_in_generate=True)\n",
    "# print(outputs)\n",
    "# # print(outputs.scores)\n",
    "# # print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "import json\n",
    "file_path = \"instruction-data.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Evaluate the following phrase by transforming it into the spelling given.', 'input': 'freind --> friend', 'output': 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Apply alpaca prompt style\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "freind --> friend\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[0])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "# Partitioning the dataset\n",
    "\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1) # 10%\n",
    "val_portion = len(data) - train_portion - test_portion # 5%\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching the dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db0c64259174027b0c4500d23d4096c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c93fb1a94b0426c98d106ecc19d45dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa5a2b05e3d4d79843730de612f5f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfe719f1b2c4fad834aa14b33833973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c518198c09543c09b8c2e948c674e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "49152\n"
     ]
    }
   ],
   "source": [
    "# Get eos token id (will be used for padding also)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-360M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=\"./.cache\")\n",
    "\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * \n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 2, 2, 2],\n",
      "        [7, 8, 9, 2, 2]]), tensor([[   1,    2,    3,    4, -100],\n",
      "        [   6,    2, -100, -100, -100],\n",
      "        [   8,    9,    2, -100, -100]]))\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_fn(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 102]) torch.Size([8, 102])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 85]) torch.Size([8, 85])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 86]) torch.Size([8, 86])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 56]) torch.Size([8, 56])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 95]) torch.Size([8, 95])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4c638bf4e3476995d01b5aa251c791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/736 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6fe4f4191d4978803f845526786937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21917969c37a4f6490baf9b98524ff18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, cache_dir=\"./.cache\", max_length=100).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lunit/home/pytholic/miniconda3/envs/llm_smollm/lib/python3.11/site-packages/transformers/generation/utils.py:1376: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Response:\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "The chef is cooking the meal every day.\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "The chef is cooking the meal every day.\n",
      "\n",
      "The chef is cooking the meal every day.\n",
      "\n",
      "The chef is cooking the meal every day.\n",
      "\n",
      "The chef is cooking the meal every day.\n",
      "\n",
      "The chef is cooking the meal every day.\n",
      "\n",
      "The chef is cooking the meal every day\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=100, temperature=0.6, top_p=0.92, do_sample=True, output_scores=True, return_dict_in_generate=True)\n",
    "outputs = tokenizer.decode(outputs.sequences[0])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    outputs = model(input_batch)\n",
    "    logits = outputs.logits \n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    eval_freq,\n",
    "    eval_iter,\n",
    "    start_context,\n",
    "    tokenizer,\n",
    "):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(\n",
    "                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = 1024\n",
    "#     inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(inputs, max_new_tokens=100, temperature=0.6, top_p=0.92, do_sample=True, output_scores=True, return_dict_in_generate=True)\n",
    "# outputs = tokenizer.decode(outputs.sequences[0])\n",
    "# print(outputs)\n",
    "    encoded = tokenizer.encode(start_context, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(encoded, max_new_tokens=100, temperature=0.6, top_p=0.92, do_sample=True)\n",
    "        decoded_text = tokenizer.decode(outputs[0])\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.4040127277374266\n",
      "Validation loss: 2.3871003150939942\n"
     ]
    }
   ],
   "source": [
    "# check the initial loss\n",
    "\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    " \n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    " \n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 1.937, Val loss 1.914\n",
      "Ep 1 (Step 000005): Train loss 0.912, Val loss 0.859\n",
      "Ep 1 (Step 000010): Train loss 0.598, Val loss 0.703\n",
      "Ep 1 (Step 000015): Train loss 0.602, Val loss 0.661\n",
      "Ep 1 (Step 000020): Train loss 0.536, Val loss 0.657\n",
      "Ep 1 (Step 000025): Train loss 0.571, Val loss 0.634\n",
      "Ep 1 (Step 000030): Train loss 0.636, Val loss 0.617\n",
      "Ep 1 (Step 000035): Train loss 0.579, Val loss 0.603\n",
      "Ep 1 (Step 000040): Train loss 0.505, Val loss 0.595\n",
      "Ep 1 (Step 000045): Train loss 0.473, Val loss 0.581\n",
      "Ep 1 (Step 000050): Train loss 0.504, Val loss 0.575\n",
      "Ep 1 (Step 000055): Train loss 0.592, Val loss 0.569\n",
      "Ep 1 (Step 000060): Train loss 0.564, Val loss 0.556\n",
      "Ep 1 (Step 000065): Train loss 0.498, Val loss 0.549\n",
      "Ep 1 (Step 000070): Train loss 0.440, Val loss 0.545\n",
      "Ep 1 (Step 000075): Train loss 0.449, Val loss 0.543\n",
      "Ep 1 (Step 000080): Train loss 0.473, Val loss 0.542\n",
      "Ep 1 (Step 000085): Train loss 0.419, Val loss 0.539\n",
      "Ep 1 (Step 000090): Train loss 0.404, Val loss 0.530\n",
      "Ep 1 (Step 000095): Train loss 0.393, Val loss 0.523\n",
      "Ep 1 (Step 000100): Train loss 0.389, Val loss 0.520\n",
      "Ep 1 (Step 000105): Train loss 0.477, Val loss 0.518\n",
      "Ep 1 (Step 000110): Train loss 0.462, Val loss 0.510\n",
      "Ep 1 (Step 000115): Train loss 0.407, Val loss 0.510\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked by the chef every day.<|im_end|>\n",
      "Ep 2 (Step 000120): Train loss 0.369, Val loss 0.511\n",
      "Ep 2 (Step 000125): Train loss 0.379, Val loss 0.515\n",
      "Ep 2 (Step 000130): Train loss 0.392, Val loss 0.515\n",
      "Ep 2 (Step 000135): Train loss 0.345, Val loss 0.513\n",
      "Ep 2 (Step 000140): Train loss 0.337, Val loss 0.512\n",
      "Ep 2 (Step 000145): Train loss 0.314, Val loss 0.511\n",
      "Ep 2 (Step 000150): Train loss 0.328, Val loss 0.505\n",
      "Ep 2 (Step 000155): Train loss 0.380, Val loss 0.501\n",
      "Ep 2 (Step 000160): Train loss 0.356, Val loss 0.500\n",
      "Ep 2 (Step 000165): Train loss 0.349, Val loss 0.496\n",
      "Ep 2 (Step 000170): Train loss 0.306, Val loss 0.495\n",
      "Ep 2 (Step 000175): Train loss 0.303, Val loss 0.496\n",
      "Ep 2 (Step 000180): Train loss 0.343, Val loss 0.494\n",
      "Ep 2 (Step 000185): Train loss 0.352, Val loss 0.491\n",
      "Ep 2 (Step 000190): Train loss 0.317, Val loss 0.487\n",
      "Ep 2 (Step 000195): Train loss 0.282, Val loss 0.479\n",
      "Ep 2 (Step 000200): Train loss 0.266, Val loss 0.476\n",
      "Ep 2 (Step 000205): Train loss 0.308, Val loss 0.473\n",
      "Ep 2 (Step 000210): Train loss 0.315, Val loss 0.469\n",
      "Ep 2 (Step 000215): Train loss 0.348, Val loss 0.470\n",
      "Ep 2 (Step 000220): Train loss 0.254, Val loss 0.481\n",
      "Ep 2 (Step 000225): Train loss 0.315, Val loss 0.488\n",
      "Ep 2 (Step 000230): Train loss 0.273, Val loss 0.486\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked by the chef every day.<|im_end|>\n",
      "Ep 3 (Step 000235): Train loss 0.302, Val loss 0.484\n",
      "Ep 3 (Step 000240): Train loss 0.291, Val loss 0.488\n",
      "Ep 3 (Step 000245): Train loss 0.282, Val loss 0.500\n",
      "Ep 3 (Step 000250): Train loss 0.249, Val loss 0.504\n",
      "Ep 3 (Step 000255): Train loss 0.243, Val loss 0.500\n",
      "Ep 3 (Step 000260): Train loss 0.271, Val loss 0.496\n",
      "Ep 3 (Step 000265): Train loss 0.245, Val loss 0.494\n",
      "Ep 3 (Step 000270): Train loss 0.236, Val loss 0.488\n",
      "Ep 3 (Step 000275): Train loss 0.244, Val loss 0.485\n",
      "Ep 3 (Step 000280): Train loss 0.243, Val loss 0.496\n",
      "Ep 3 (Step 000285): Train loss 0.273, Val loss 0.501\n",
      "Ep 3 (Step 000290): Train loss 0.259, Val loss 0.507\n",
      "Ep 3 (Step 000295): Train loss 0.241, Val loss 0.498\n",
      "Ep 3 (Step 000300): Train loss 0.236, Val loss 0.488\n",
      "Ep 3 (Step 000305): Train loss 0.235, Val loss 0.487\n",
      "Ep 3 (Step 000310): Train loss 0.246, Val loss 0.487\n",
      "Ep 3 (Step 000315): Train loss 0.226, Val loss 0.481\n",
      "Ep 3 (Step 000320): Train loss 0.231, Val loss 0.480\n",
      "Ep 3 (Step 000325): Train loss 0.210, Val loss 0.484\n",
      "Ep 3 (Step 000330): Train loss 0.212, Val loss 0.482\n",
      "Ep 3 (Step 000335): Train loss 0.213, Val loss 0.485\n",
      "Ep 3 (Step 000340): Train loss 0.230, Val loss 0.487\n",
      "Ep 3 (Step 000345): Train loss 0.250, Val loss 0.487\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked by the chef every day.<|im_end|>\n",
      "Ep 4 (Step 000350): Train loss 0.217, Val loss 0.491\n",
      "Ep 4 (Step 000355): Train loss 0.194, Val loss 0.510\n",
      "Ep 4 (Step 000360): Train loss 0.218, Val loss 0.523\n",
      "Ep 4 (Step 000365): Train loss 0.227, Val loss 0.517\n",
      "Ep 4 (Step 000370): Train loss 0.242, Val loss 0.508\n",
      "Ep 4 (Step 000375): Train loss 0.235, Val loss 0.503\n",
      "Ep 4 (Step 000380): Train loss 0.195, Val loss 0.508\n",
      "Ep 4 (Step 000385): Train loss 0.205, Val loss 0.515\n",
      "Ep 4 (Step 000390): Train loss 0.205, Val loss 0.522\n",
      "Ep 4 (Step 000395): Train loss 0.183, Val loss 0.524\n",
      "Ep 4 (Step 000400): Train loss 0.194, Val loss 0.520\n",
      "Ep 4 (Step 000405): Train loss 0.196, Val loss 0.518\n",
      "Ep 4 (Step 000410): Train loss 0.190, Val loss 0.507\n",
      "Ep 4 (Step 000415): Train loss 0.213, Val loss 0.502\n",
      "Ep 4 (Step 000420): Train loss 0.195, Val loss 0.504\n",
      "Ep 4 (Step 000425): Train loss 0.194, Val loss 0.508\n",
      "Ep 4 (Step 000430): Train loss 0.193, Val loss 0.510\n",
      "Ep 4 (Step 000435): Train loss 0.207, Val loss 0.513\n",
      "Ep 4 (Step 000440): Train loss 0.179, Val loss 0.513\n",
      "Ep 4 (Step 000445): Train loss 0.176, Val loss 0.504\n",
      "Ep 4 (Step 000450): Train loss 0.172, Val loss 0.497\n",
      "Ep 4 (Step 000455): Train loss 0.185, Val loss 0.501\n",
      "Ep 4 (Step 000460): Train loss 0.200, Val loss 0.508\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked by the chef every day.<|im_end|>\n",
      "Ep 5 (Step 000465): Train loss 0.197, Val loss 0.507\n",
      "Ep 5 (Step 000470): Train loss 0.180, Val loss 0.514\n",
      "Ep 5 (Step 000475): Train loss 0.179, Val loss 0.528\n",
      "Ep 5 (Step 000480): Train loss 0.178, Val loss 0.530\n",
      "Ep 5 (Step 000485): Train loss 0.183, Val loss 0.527\n",
      "Ep 5 (Step 000490): Train loss 0.177, Val loss 0.526\n",
      "Ep 5 (Step 000495): Train loss 0.184, Val loss 0.515\n",
      "Ep 5 (Step 000500): Train loss 0.176, Val loss 0.510\n",
      "Ep 5 (Step 000505): Train loss 0.189, Val loss 0.513\n",
      "Ep 5 (Step 000510): Train loss 0.180, Val loss 0.518\n",
      "Ep 5 (Step 000515): Train loss 0.163, Val loss 0.525\n",
      "Ep 5 (Step 000520): Train loss 0.180, Val loss 0.527\n",
      "Ep 5 (Step 000525): Train loss 0.185, Val loss 0.528\n",
      "Ep 5 (Step 000530): Train loss 0.181, Val loss 0.522\n",
      "Ep 5 (Step 000535): Train loss 0.154, Val loss 0.517\n",
      "Ep 5 (Step 000540): Train loss 0.173, Val loss 0.515\n",
      "Ep 5 (Step 000545): Train loss 0.168, Val loss 0.510\n",
      "Ep 5 (Step 000550): Train loss 0.167, Val loss 0.513\n",
      "Ep 5 (Step 000555): Train loss 0.171, Val loss 0.515\n",
      "Ep 5 (Step 000560): Train loss 0.170, Val loss 0.522\n",
      "Ep 5 (Step 000565): Train loss 0.167, Val loss 0.522\n",
      "Ep 5 (Step 000570): Train loss 0.174, Val loss 0.526\n",
      "Ep 5 (Step 000575): Train loss 0.180, Val loss 0.523\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked by the chef every day.<|im_end|>\n",
      "Training completed in 3.74 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "import time\n",
    " \n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00004, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    " \n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    " \n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Rewrite the sentence using a simile.  ### Input: The car is very fast.  ### Response: The car is as fast as a cheetah.<|im_end|>\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What type of cloud is typically associated with thunderstorms?  ### Response: The type of cloud typically associated with thunderstorms is cumulus clouds.<|im_end|>\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulus clouds.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Name the author of 'Pride and Prejudice'.  ### Response: The author of 'Pride and Prejudice' is Jane Austen.<|im_end|>\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test some responses\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "    encoded = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(encoded, max_new_tokens=256, do_sample=False)\n",
    "    # outputs = model.generate(encoded, max_new_tokens=256, temperature=0.6, top_p=0.92, do_sample=True)\n",
    "    decoded_text = tokenizer.decode(outputs[0])\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    response_text = (\n",
    "        decoded_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .replace(tokenizer.eos_token, \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model and responses for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 110/110 [00:48<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Save all responses for future evaluations\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "    encoded = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(encoded, max_new_tokens=256, do_sample=False)\n",
    "    decoded_text = tokenizer.decode(outputs[0])\n",
    "    response_text = (\n",
    "        decoded_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .replace(tokenizer.eos_token, \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# file_name = \"smollm-sft-epoch-5.pth\"\n",
    "# torch.save(model.state_dict(), file_name)\n",
    "# print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
